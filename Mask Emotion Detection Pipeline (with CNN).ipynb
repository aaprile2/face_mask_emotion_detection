{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Mask Emotion Detection Pipeline (with CNN).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMDaA35tlIW26o4UE+stte0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ONQKG-KGGC5o"},"source":["Aprile, Amezquita, Schaber <br>CPE-695 Final Project <br>Facial Expression Detection with Limited Features"]},{"cell_type":"markdown","metadata":{"id":"LtFgaTVrG7l3"},"source":["# <font color=navy> **Mask Emotion Detection Pipeline**\n","This code combines the prior efforts of Dlib (facial feature detection) and the model building (facial emotion detection). Given an image, it saves the image annotated with facial bounding boxes and the emotion, as predicted by the CNN, whose architecture and weights have been loaded."]},{"cell_type":"code","metadata":{"id":"llHmPAIl41pu"},"source":["# Imports\n","from imutils import face_utils\n","from google.colab import drive\n","import numpy as np\n","import dlib\n","import cv2\n","import glob\n","import os\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from keras.models import model_from_json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wUM3i7dZ5g0Y","executionInfo":{"status":"ok","timestamp":1608131299147,"user_tz":300,"elapsed":23184,"user":{"displayName":"Allison Aprile","photoUrl":"","userId":"13865243459094037113"}},"outputId":"eec2f7ea-9b69-4071-cd56-ff034103b691"},"source":["# Mount the drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M6X-lwK-7_iL"},"source":["# Load CNN for prediction\n","json_file = open('/content/drive/My Drive/CPE-695 Final Project/Data/CNN_model_architecture.json', 'r')\n","model_json = json_file.read()\n","json_file.close()\n","model = model_from_json(model_json)\n","\n","# Load weights\n","model.load_weights('/content/drive/My Drive/CPE-695 Final Project/Data/CNN_model_weights.h5')\n","\n","# Compile model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1nr9ScAB8EF0"},"source":["# Prediction function\n","def predict_emotion(face, model):\n","  # Define emotions\n","  emotions = {\n","    0 : 'Angry',\n","    1: 'Disgust',\n","    2: 'Fear',\n","    3: 'Happy',\n","    4: 'Sad',\n","    5: 'Surprise',\n","    6: 'Neutral'\n","  }\n","\n","  # Build and normalize input tensor\n","  X = face.reshape(1, 29, 48, 1).astype('float32') / 255\n","\n","  # Predict\n","  pred = model.predict(X)\n","\n","  # Return emotion\n","  return emotions[np.argmax(pred, axis=1)[0]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vx3KSneDE1Fi"},"source":["# Label image\n","def label_image(image, path):\n","  # Convert to grayscale\n","  gray = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2GRAY)\n","\n","  # Load predictor\n","  predictor = dlib.shape_predictor(\"/content/drive/My Drive/CPE-695 Final Project/Data/shape_predictor_68_face_landmarks.dat\")\n","  detector = dlib.get_frontal_face_detector()\n","\n","  # Get facial bounding boxes\n","  rects = detector(gray, 1)\n","\n","  # Crop to custom images\n","  faces = []\n","\n","  for rect in rects:\n","    x1 = rect.left()\n","    y1 = rect.top()\n","    x2 = rect.right()\n","    y2 = rect.bottom()\n","\n","    faces.append(cv2.resize(gray[y1:y2, x1:x2], (48,48)))\n","\n","  for f in range(len(faces)):\n","    img = faces[f]\n","\n","    # Define new bounding box\n","    # Single faces are bounded, use top left and bottom right points\n","    bb = dlib.rectangle(0,0, img.shape[0], img.shape[1])\n","\n","    # Further define box points\n","    x1 = bb.left()\n","    y1 = bb.top()\n","    x2 = bb.right() \n","    y2 = bb.bottom() \n","\n","    # Get y-coordinate of nose (point 31)\n","    nose_y = predictor(image=img, box=bb).part(30).y\n","\n","    # Crop image in faces\n","    cropped = img[0:nose_y, :]\n","\n","    # Reshape (for input in model)\n","    faces[f] = cv2.resize(cropped, (29, 48))\n","\n","  # Make predictions and draw on image\n","  for ind in range(len(faces)):\n","    # Predict emotion\n","    emotion = predict_emotion(faces[ind], model)\n","\n","    # Draw face bounding box\n","    (x, y, w, h) = face_utils.rect_to_bb(rects[ind])\n","    cv2.rectangle(image, (x,y), (x+w, y+h), (255,0,0), 3)\n","    cv2.putText(image, emotion, (x-10,y-10), cv2.FONT_HERSHEY_DUPLEX, 1.5, (255,0,0), 2)\n","    \n","    # Save image\n","    cv2.imwrite(path, image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JG1wYmscGzm4"},"source":["### **<font color=maroon>Input images here!**"]},{"cell_type":"code","metadata":{"id":"mO6btVdQHn9t"},"source":["# Annotate six test images\n","tests = glob.glob('/content/drive/My Drive/CPE-695 Final Project/Test Images/*.jpg')\n","save_path = '/content/drive/My Drive/CPE-695 Final Project/Test Images/annotated_'\n","\n","for t in tests:\n","  image = cv2.imread(t)\n","  label_image(image, save_path + t.split('/')[-1])"],"execution_count":null,"outputs":[]}]}